net output size:
torch.Size([8, 751])
[Resize(size=(384, 192), interpolation=PIL.Image.BICUBIC), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), <random_erasing.RandomErasing object at 0x7f6363638588>]
/opt/anaconda3/lib/python3.7/site-packages/torchvision-0.2.1-py3.7.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:14: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  init.kaiming_normal(m.weight.data, a=0, mode='fan_out')
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:15: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:23: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, std=0.001)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:24: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
0.38144421577453613
PCB(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 1))
  (dropout): Dropout(p=0.5)
  (classifier0): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier1): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier2): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier3): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier4): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier5): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 37.4119 Acc: 0.0599
val Loss: 32.6649 Acc: 0.0907

Epoch 1/59
----------
train Loss: 28.7736 Acc: 0.1990
val Loss: 26.8318 Acc: 0.2340

Epoch 2/59
----------
train Loss: 24.1525 Acc: 0.3370
val Loss: 22.2181 Acc: 0.3453

Epoch 3/59
----------
train Loss: 21.2010 Acc: 0.4354
val Loss: 19.6200 Acc: 0.4320

Epoch 4/59
----------
train Loss: 19.0446 Acc: 0.5258
val Loss: 17.9158 Acc: 0.4860

Epoch 5/59
----------
train Loss: 17.5396 Acc: 0.5833
val Loss: 17.1671 Acc: 0.5067

Epoch 6/59
----------
train Loss: 16.3400 Acc: 0.6285
val Loss: 14.8180 Acc: 0.6000

Epoch 7/59
----------
train Loss: 15.3625 Acc: 0.6654
val Loss: 14.4828 Acc: 0.6013

Epoch 8/59
----------
train Loss: 14.6085 Acc: 0.6937
val Loss: 12.7994 Acc: 0.6660

Epoch 9/59
----------
train Loss: 13.9233 Acc: 0.7196
val Loss: 13.6450 Acc: 0.6380

Epoch 10/59
----------
train Loss: 13.3076 Acc: 0.7372
val Loss: 12.2704 Acc: 0.6780

Epoch 11/59
----------
train Loss: 12.9303 Acc: 0.7546
val Loss: 11.9512 Acc: 0.6987

Epoch 12/59
----------
train Loss: 12.4223 Acc: 0.7720
val Loss: 11.3022 Acc: 0.7347

Epoch 13/59
----------
train Loss: 12.0570 Acc: 0.7844
val Loss: 10.6611 Acc: 0.7380

Epoch 14/59
----------
train Loss: 11.6606 Acc: 0.7982
val Loss: 10.7605 Acc: 0.7373

Epoch 15/59
----------
train Loss: 11.3226 Acc: 0.8080
val Loss: 10.0079 Acc: 0.7700

Epoch 16/59
----------
train Loss: 11.1148 Acc: 0.8164
val Loss: 10.0018 Acc: 0.7700

Epoch 17/59
----------
train Loss: 10.7748 Acc: 0.8284
val Loss: 9.8372 Acc: 0.7673

Epoch 18/59
----------
train Loss: 10.5835 Acc: 0.8298
val Loss: 10.0454 Acc: 0.7653

Epoch 19/59
----------
train Loss: 10.3299 Acc: 0.8395
val Loss: 10.1270 Acc: 0.7707

Epoch 20/59
----------
train Loss: 10.1158 Acc: 0.8485
val Loss: 9.6861 Acc: 0.7787

Epoch 21/59
----------
train Loss: 9.8983 Acc: 0.8547
val Loss: 9.3409 Acc: 0.7827

Epoch 22/59
----------
train Loss: 9.6726 Acc: 0.8601
val Loss: 9.2664 Acc: 0.7860

Epoch 23/59
----------
train Loss: 9.5506 Acc: 0.8640
val Loss: 9.3167 Acc: 0.7893

Epoch 24/59
----------
train Loss: 9.3289 Acc: 0.8688
val Loss: 9.4818 Acc: 0.7800

Epoch 25/59
----------
train Loss: 9.1893 Acc: 0.8762
val Loss: 8.9989 Acc: 0.8040

Epoch 26/59
----------
train Loss: 9.0869 Acc: 0.8814
val Loss: 8.8349 Acc: 0.8053

Epoch 27/59
----------
train Loss: 8.9928 Acc: 0.8764
val Loss: 8.7856 Acc: 0.8107

Epoch 28/59
----------
train Loss: 8.7738 Acc: 0.8845
val Loss: 8.6251 Acc: 0.8127

Epoch 29/59
----------
train Loss: 8.7288 Acc: 0.8865
val Loss: 8.6307 Acc: 0.8133

Epoch 30/59
----------
train Loss: 8.5529 Acc: 0.8902
val Loss: 8.6214 Acc: 0.8080

Epoch 31/59
----------
train Loss: 8.5042 Acc: 0.8923
val Loss: 8.7409 Acc: 0.8060

Epoch 32/59
----------
train Loss: 8.4068 Acc: 0.8934
val Loss: 8.1763 Acc: 0.8340

Epoch 33/59
----------
train Loss: 8.2610 Acc: 0.8979
val Loss: 8.3467 Acc: 0.8240

Epoch 34/59
----------
train Loss: 8.1644 Acc: 0.9016
val Loss: 8.5142 Acc: 0.8100

Epoch 35/59
----------
train Loss: 8.0762 Acc: 0.9032
val Loss: 8.5495 Acc: 0.8120

Epoch 36/59
----------
train Loss: 7.9504 Acc: 0.9055
val Loss: 9.2955 Acc: 0.8053

Epoch 37/59
----------
train Loss: 7.8983 Acc: 0.9071
val Loss: 8.4993 Acc: 0.8167

Epoch 38/59
----------
train Loss: 7.8325 Acc: 0.9086
val Loss: 8.1276 Acc: 0.8300

Epoch 39/59
----------
train Loss: 7.6727 Acc: 0.9120
val Loss: 8.2328 Acc: 0.8253

Epoch 40/59
----------
train Loss: 3.9439 Acc: 0.9715
val Loss: 4.0467 Acc: 0.9207

Epoch 41/59
----------
train Loss: 2.5777 Acc: 0.9876
val Loss: 3.6261 Acc: 0.9253

Epoch 42/59
----------
train Loss: 2.1715 Acc: 0.9910
val Loss: 3.4420 Acc: 0.9353

Epoch 43/59
----------
train Loss: 1.9352 Acc: 0.9934
val Loss: 3.3298 Acc: 0.9340

Epoch 44/59
----------
train Loss: 1.7647 Acc: 0.9940
val Loss: 3.2546 Acc: 0.9373

Epoch 45/59
----------
train Loss: 1.6883 Acc: 0.9947
val Loss: 3.1511 Acc: 0.9420

Epoch 46/59
----------
train Loss: 1.5756 Acc: 0.9955
val Loss: 3.1056 Acc: 0.9373

Epoch 47/59
----------
train Loss: 1.5069 Acc: 0.9956
val Loss: 3.1430 Acc: 0.9373

Epoch 48/59
----------
train Loss: 1.4575 Acc: 0.9957
val Loss: 3.0413 Acc: 0.9433

Epoch 49/59
----------
train Loss: 1.3592 Acc: 0.9965
val Loss: 3.0484 Acc: 0.9433

Epoch 50/59
----------
train Loss: 1.3351 Acc: 0.9967
val Loss: 3.0425 Acc: 0.9387

Epoch 51/59
----------
train Loss: 1.3086 Acc: 0.9970
val Loss: 3.0663 Acc: 0.9400

Epoch 52/59
----------
train Loss: 1.2718 Acc: 0.9968
val Loss: 2.9915 Acc: 0.9427

Epoch 53/59
----------
train Loss: 1.2362 Acc: 0.9969
val Loss: 2.9549 Acc: 0.9407

Epoch 54/59
----------
train Loss: 1.2125 Acc: 0.9973
val Loss: 3.0097 Acc: 0.9400

Epoch 55/59
----------
train Loss: 1.2216 Acc: 0.9971
val Loss: 3.0210 Acc: 0.9380

Epoch 56/59
----------
train Loss: 1.1846 Acc: 0.9973
val Loss: 2.9609 Acc: 0.9427

Epoch 57/59
----------
train Loss: 1.1627 Acc: 0.9973
val Loss: 2.9830 Acc: 0.9427

Epoch 58/59
----------
train Loss: 1.1498 Acc: 0.9977
val Loss: 2.9823 Acc: 0.9427

Epoch 59/59
----------
train Loss: 1.1596 Acc: 0.9968
val Loss: 3.0197 Acc: 0.9467

Training complete in 293m 29s
