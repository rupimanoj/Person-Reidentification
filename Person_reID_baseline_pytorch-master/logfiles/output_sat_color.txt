net output size:
torch.Size([8, 751])
[ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0), Resize(size=(384, 192), interpolation=PIL.Image.BICUBIC), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
/opt/anaconda3/lib/python3.7/site-packages/torchvision-0.2.1-py3.7.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:14: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  init.kaiming_normal(m.weight.data, a=0, mode='fan_out')
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:15: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, 1.0, 0.02)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:18: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:23: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.
  init.normal(m.weight.data, std=0.001)
/home/janardhan/AI_Person_ReID/Person_reID_baseline_pytorch-master/model.py:24: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  init.constant(m.bias.data, 0.0)
0.48154187202453613
PCB(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 1))
  (dropout): Dropout(p=0.5)
  (classifier0): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier1): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier2): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier3): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier4): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
  (classifier5): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=256, bias=True)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=256, out_features=1500, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 36.2290 Acc: 0.0788
val Loss: 30.9587 Acc: 0.1360

Epoch 1/59
----------
train Loss: 26.9001 Acc: 0.2536
val Loss: 26.0889 Acc: 0.2433

Epoch 2/59
----------
train Loss: 22.1760 Acc: 0.4016
val Loss: 21.6239 Acc: 0.3587

Epoch 3/59
----------
train Loss: 19.0300 Acc: 0.5180
val Loss: 19.2094 Acc: 0.4387

Epoch 4/59
----------
train Loss: 16.8823 Acc: 0.6048
val Loss: 16.4076 Acc: 0.5547

Epoch 5/59
----------
train Loss: 15.2688 Acc: 0.6645
val Loss: 14.6719 Acc: 0.5987

Epoch 6/59
----------
train Loss: 14.0794 Acc: 0.7068
val Loss: 14.1752 Acc: 0.6333

Epoch 7/59
----------
train Loss: 13.1379 Acc: 0.7436
val Loss: 13.3533 Acc: 0.6520

Epoch 8/59
----------
train Loss: 12.3643 Acc: 0.7685
val Loss: 12.3137 Acc: 0.7107

Epoch 9/59
----------
train Loss: 11.6656 Acc: 0.7924
val Loss: 11.9081 Acc: 0.7060

Epoch 10/59
----------
train Loss: 11.0219 Acc: 0.8149
val Loss: 12.0505 Acc: 0.7193

Epoch 11/59
----------
train Loss: 10.6088 Acc: 0.8269
val Loss: 11.3667 Acc: 0.7340

Epoch 12/59
----------
train Loss: 10.1713 Acc: 0.8443
val Loss: 11.2955 Acc: 0.7440

Epoch 13/59
----------
train Loss: 9.7267 Acc: 0.8562
val Loss: 10.6086 Acc: 0.7613

Epoch 14/59
----------
train Loss: 9.3843 Acc: 0.8653
val Loss: 11.3095 Acc: 0.7387

Epoch 15/59
----------
train Loss: 9.0331 Acc: 0.8758
val Loss: 10.6578 Acc: 0.7560

Epoch 16/59
----------
train Loss: 8.7710 Acc: 0.8856
val Loss: 9.4949 Acc: 0.7960

Epoch 17/59
----------
train Loss: 8.4879 Acc: 0.8903
val Loss: 9.3784 Acc: 0.7960

Epoch 18/59
----------
train Loss: 8.2107 Acc: 0.8960
val Loss: 9.8212 Acc: 0.7827

Epoch 19/59
----------
train Loss: 8.0280 Acc: 0.9025
val Loss: 9.5104 Acc: 0.7893

Epoch 20/59
----------
train Loss: 7.8304 Acc: 0.9084
val Loss: 9.3430 Acc: 0.8080

Epoch 21/59
----------
train Loss: 7.6405 Acc: 0.9135
val Loss: 9.2421 Acc: 0.8027

Epoch 22/59
----------
train Loss: 7.4747 Acc: 0.9172
val Loss: 10.1444 Acc: 0.7867

Epoch 23/59
----------
train Loss: 7.2066 Acc: 0.9225
val Loss: 8.8860 Acc: 0.8100

Epoch 24/59
----------
train Loss: 7.0888 Acc: 0.9259
val Loss: 10.0700 Acc: 0.7940

Epoch 25/59
----------
train Loss: 6.9969 Acc: 0.9299
val Loss: 9.3517 Acc: 0.7987

Epoch 26/59
----------
train Loss: 6.8180 Acc: 0.9347
val Loss: 8.9639 Acc: 0.8107

Epoch 27/59
----------
train Loss: 6.6114 Acc: 0.9358
val Loss: 9.5667 Acc: 0.7987

Epoch 28/59
----------
train Loss: 6.4970 Acc: 0.9374
val Loss: 8.3621 Acc: 0.8307

Epoch 29/59
----------
train Loss: 6.3892 Acc: 0.9392
val Loss: 8.7396 Acc: 0.8193

Epoch 30/59
----------
train Loss: 6.2557 Acc: 0.9438
val Loss: 8.5582 Acc: 0.8293

Epoch 31/59
----------
train Loss: 6.2113 Acc: 0.9432
val Loss: 8.4751 Acc: 0.8280

Epoch 32/59
----------
train Loss: 6.0589 Acc: 0.9505
val Loss: 8.6229 Acc: 0.8380

Epoch 33/59
----------
train Loss: 5.9873 Acc: 0.9498
val Loss: 8.4413 Acc: 0.8347

Epoch 34/59
----------
train Loss: 5.9112 Acc: 0.9524
val Loss: 8.8145 Acc: 0.8240

Epoch 35/59
----------
train Loss: 5.7243 Acc: 0.9524
val Loss: 8.8743 Acc: 0.8307

Epoch 36/59
----------
train Loss: 5.7263 Acc: 0.9545
val Loss: 8.7425 Acc: 0.8327

Epoch 37/59
----------
train Loss: 5.5988 Acc: 0.9574
val Loss: 8.5830 Acc: 0.8207

Epoch 38/59
----------
train Loss: 5.6260 Acc: 0.9546
val Loss: 8.7656 Acc: 0.8253

Epoch 39/59
----------
train Loss: 5.4153 Acc: 0.9611
val Loss: 9.1782 Acc: 0.8153

Epoch 40/59
----------
train Loss: 2.1936 Acc: 0.9910
val Loss: 4.3682 Acc: 0.9200

Epoch 41/59
----------
train Loss: 1.1163 Acc: 0.9978
val Loss: 4.0171 Acc: 0.9220

Epoch 42/59
----------
train Loss: 0.8573 Acc: 0.9987
val Loss: 3.7413 Acc: 0.9300

Epoch 43/59
----------
train Loss: 0.7234 Acc: 0.9989
val Loss: 3.6866 Acc: 0.9300

Epoch 44/59
----------
train Loss: 0.6251 Acc: 0.9992
val Loss: 3.6624 Acc: 0.9287

Epoch 45/59
----------
train Loss: 0.5677 Acc: 0.9991
val Loss: 3.5983 Acc: 0.9340

Epoch 46/59
----------
train Loss: 0.5295 Acc: 0.9994
val Loss: 3.5344 Acc: 0.9340

Epoch 47/59
----------
train Loss: 0.4927 Acc: 0.9993
val Loss: 3.4555 Acc: 0.9353

Epoch 48/59
----------
train Loss: 0.4665 Acc: 0.9995
val Loss: 3.4787 Acc: 0.9360

Epoch 49/59
----------
train Loss: 0.4500 Acc: 0.9995
val Loss: 3.4740 Acc: 0.9373

Epoch 50/59
----------
